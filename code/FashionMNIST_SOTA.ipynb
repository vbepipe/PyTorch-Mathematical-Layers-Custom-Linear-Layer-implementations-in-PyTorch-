{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv8yMG4QOpW0"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple, Dict, Any, Optional, List\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torch.amp import autocast, GradScaler\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import copy\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================================\n",
        "# GPU DEVICE SETUP FOR T4\n",
        "# ============================================================================\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory Used: {torch.cuda.memory_allocated(0) / 1024**3:.1f} GB\")\n",
        "        print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1024**3:.1f} GB\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCED CONFIGURATION FOR 99% TARGET\n",
        "# ============================================================================\n",
        "\n",
        "DATASET_NAMES = ['FashionMNIST']\n",
        "HIDDEN_DIM: int = 3072\n",
        "LEARNING_RATE: float = 8e-4\n",
        "weight_decay_ = 2e-4\n",
        "DROPOUT_RATE: float = 0.3\n",
        "EPOCHS_PER_DATASET: int = 200\n",
        "BATCH_SIZE: int = 512\n",
        "ENSEMBLE_SIZE: int = 7\n",
        "TTA_AUGMENTS: int = 8\n",
        "\n",
        "# Checkpoint configuration\n",
        "CHECKPOINT_DIR = \"./checkpoints\"\n",
        "SAVE_TOP_K = 3  # Keep top 3 best models\n",
        "SAVE_EVERY_N_EPOCHS = 5  # Save checkpoint every N epochs\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL CHECKPOINTING SYSTEM\n",
        "# ============================================================================\n",
        "\n",
        "class ModelCheckpoint:\n",
        "    \"\"\"Advanced model checkpointing system\"\"\"\n",
        "\n",
        "    def __init__(self, checkpoint_dir: str = CHECKPOINT_DIR, save_top_k: int = SAVE_TOP_K):\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.save_top_k = save_top_k\n",
        "        self.best_models = []  # List of (accuracy, checkpoint_path) tuples\n",
        "\n",
        "    def save_checkpoint(self, model, optimizer, scheduler, epoch, accuracy, loss,\n",
        "                       model_name=\"model\", additional_info=None):\n",
        "        \"\"\"Save model checkpoint with all training state\"\"\"\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        checkpoint_name = f\"{model_name}_epoch{epoch}_acc{accuracy:.2f}_{timestamp}.pt\"\n",
        "        checkpoint_path = self.checkpoint_dir / checkpoint_name\n",
        "\n",
        "        # Prepare checkpoint data\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'accuracy': accuracy,\n",
        "            'loss': loss,\n",
        "            'model_name': model_name,\n",
        "            'timestamp': timestamp,\n",
        "            'model_architecture': model.__class__.__name__,\n",
        "            'random_state': {\n",
        "                'torch': torch.get_rng_state(),\n",
        "                'numpy': np.random.get_state(),\n",
        "                'python': random.getstate(),\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            checkpoint['cuda_random_state'] = torch.cuda.get_rng_state()\n",
        "\n",
        "        if additional_info:\n",
        "            checkpoint['additional_info'] = additional_info\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"‚úÖ Checkpoint saved: {checkpoint_name} (Acc: {accuracy:.2f}%)\")\n",
        "\n",
        "        # Update best models list\n",
        "        self.best_models.append((accuracy, checkpoint_path, checkpoint_name))\n",
        "        self.best_models.sort(key=lambda x: x[0], reverse=True)  # Sort by accuracy desc\n",
        "\n",
        "        # Keep only top-k models\n",
        "        if len(self.best_models) > self.save_top_k:\n",
        "            # Delete old checkpoints\n",
        "            for _, old_path, old_name in self.best_models[self.save_top_k:]:\n",
        "                if old_path.exists():\n",
        "                    old_path.unlink()\n",
        "                    print(f\"üóëÔ∏è Removed old checkpoint: {old_name}\")\n",
        "            self.best_models = self.best_models[:self.save_top_k]\n",
        "\n",
        "        # Save best models info\n",
        "        self.save_best_models_info()\n",
        "\n",
        "        return checkpoint_path\n",
        "\n",
        "    def save_best_models_info(self):\n",
        "        \"\"\"Save information about best models\"\"\"\n",
        "        best_models_info = {\n",
        "            'best_models': [\n",
        "                {\n",
        "                    'accuracy': acc,\n",
        "                    'checkpoint_path': str(path),\n",
        "                    'checkpoint_name': name,\n",
        "                }\n",
        "                for acc, path, name in self.best_models\n",
        "            ],\n",
        "            'last_updated': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        info_path = self.checkpoint_dir / \"best_models_info.json\"\n",
        "        with open(info_path, 'w') as f:\n",
        "            json.dump(best_models_info, f, indent=2)\n",
        "\n",
        "    def load_best_checkpoint(self, model, optimizer=None, scheduler=None):\n",
        "        \"\"\"Load the best checkpoint\"\"\"\n",
        "        if not self.best_models:\n",
        "            self.load_best_models_info()\n",
        "\n",
        "        if not self.best_models:\n",
        "            print(\"‚ö†Ô∏è No checkpoints found\")\n",
        "            return None\n",
        "\n",
        "        best_accuracy, best_path, best_name = self.best_models[0]\n",
        "        return self.load_checkpoint(best_path, model, optimizer, scheduler)\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path, model, optimizer=None, scheduler=None):\n",
        "        \"\"\"Load specific checkpoint\"\"\"\n",
        "        checkpoint_path = Path(checkpoint_path)\n",
        "\n",
        "        if not checkpoint_path.exists():\n",
        "            print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "            # Load model state\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "            # Load optimizer state\n",
        "            if optimizer and 'optimizer_state_dict' in checkpoint:\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "            # Load scheduler state\n",
        "            if scheduler and 'scheduler_state_dict' in checkpoint:\n",
        "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "            # Restore random states\n",
        "            if 'random_state' in checkpoint:\n",
        "                torch.set_rng_state(checkpoint['random_state']['torch'])\n",
        "                np.random.set_state(checkpoint['random_state']['numpy'])\n",
        "                random.setstate(checkpoint['random_state']['python'])\n",
        "\n",
        "                if torch.cuda.is_available() and 'cuda_random_state' in checkpoint:\n",
        "                    torch.cuda.set_rng_state(checkpoint['cuda_random_state'])\n",
        "\n",
        "            print(f\"‚úÖ Loaded checkpoint: {checkpoint_path.name}\")\n",
        "            print(f\"   Epoch: {checkpoint['epoch']}, Accuracy: {checkpoint['accuracy']:.2f}%\")\n",
        "\n",
        "            return {\n",
        "                'epoch': checkpoint['epoch'],\n",
        "                'accuracy': checkpoint['accuracy'],\n",
        "                'loss': checkpoint['loss'],\n",
        "                'model_name': checkpoint.get('model_name', 'unknown'),\n",
        "                'timestamp': checkpoint.get('timestamp', 'unknown')\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading checkpoint {checkpoint_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_best_models_info(self):\n",
        "        \"\"\"Load information about best models\"\"\"\n",
        "        info_path = self.checkpoint_dir / \"best_models_info.json\"\n",
        "\n",
        "        if info_path.exists():\n",
        "            try:\n",
        "                with open(info_path, 'r') as f:\n",
        "                    info = json.load(f)\n",
        "\n",
        "                self.best_models = []\n",
        "                for model_info in info['best_models']:\n",
        "                    path = Path(model_info['checkpoint_path'])\n",
        "                    if path.exists():\n",
        "                        self.best_models.append((\n",
        "                            model_info['accuracy'],\n",
        "                            path,\n",
        "                            model_info['checkpoint_name']\n",
        "                        ))\n",
        "\n",
        "                print(f\"üìÇ Loaded {len(self.best_models)} checkpoint(s) from history\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading checkpoint info: {e}\")\n",
        "\n",
        "    def list_checkpoints(self):\n",
        "        \"\"\"List all available checkpoints\"\"\"\n",
        "        if not self.best_models:\n",
        "            self.load_best_models_info()\n",
        "\n",
        "        if not self.best_models:\n",
        "            print(\"No checkpoints found\")\n",
        "            return\n",
        "\n",
        "        print(\"\\nüìã Available Checkpoints:\")\n",
        "        print(\"-\" * 60)\n",
        "        for i, (accuracy, path, name) in enumerate(self.best_models):\n",
        "            print(f\"{i+1}. {name}\")\n",
        "            print(f\"   Accuracy: {accuracy:.2f}%\")\n",
        "            print(f\"   Path: {path}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "    def get_resume_info(self):\n",
        "        \"\"\"Get information for resuming training\"\"\"\n",
        "        if not self.best_models:\n",
        "            self.load_best_models_info()\n",
        "\n",
        "        if not self.best_models:\n",
        "            return None\n",
        "\n",
        "        best_accuracy, best_path, best_name = self.best_models[0]\n",
        "\n",
        "        return {\n",
        "            'checkpoint_path': best_path,\n",
        "            'checkpoint_name': best_name,\n",
        "            'best_accuracy': best_accuracy\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# ADVANCED DATA AUGMENTATION STRATEGIES\n",
        "# ============================================================================\n",
        "\n",
        "class CutMix(object):\n",
        "    \"\"\"CutMix augmentation\"\"\"\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        if self.alpha <= 0:\n",
        "            return x, y, y, 1.0\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        lam = np.random.beta(self.alpha, self.alpha)\n",
        "        index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "        bbx1, bby1, bbx2, bby2 = self._rand_bbox(x.size(), lam)\n",
        "        x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
        "        y_a, y_b = y, y[index]\n",
        "        return x, y_a, y_b, lam\n",
        "\n",
        "    def _rand_bbox(self, size, lam):\n",
        "        W = size[2]\n",
        "        H = size[3]\n",
        "        cut_rat = np.sqrt(1. - lam)\n",
        "        cut_w = np.int32(W * cut_rat)\n",
        "        cut_h = np.int32(H * cut_rat)\n",
        "\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "class MixUp(object):\n",
        "    \"\"\"Enhanced MixUp with adaptive alpha\"\"\"\n",
        "    def __init__(self, alpha=0.4):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        if self.alpha > 0:\n",
        "            lam = np.random.beta(self.alpha, self.alpha)\n",
        "        else:\n",
        "            lam = 1\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "        y_a, y_b = y, y[index]\n",
        "        return mixed_x, y_a, y_b, lam\n",
        "\n",
        "class AdvancedAugmentDataset(Dataset):\n",
        "    \"\"\"Advanced dataset with sophisticated augmentations\"\"\"\n",
        "    def __init__(self, base_dataset, num_augments=3, heavy_aug=True):\n",
        "        self.base = base_dataset\n",
        "        self.num_augments = num_augments\n",
        "        self.heavy_aug = heavy_aug\n",
        "\n",
        "        if heavy_aug:\n",
        "            self.aug_transform = transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomRotation(20),\n",
        "                transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.8, 1.2)),\n",
        "                transforms.RandomResizedCrop(28, scale=(0.7, 1.0)),\n",
        "                transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "                transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,), (0.5,)),\n",
        "                transforms.RandomErasing(p=0.6, scale=(0.02, 0.4), ratio=(0.3, 3.3)),\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base) * self.num_augments\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_idx = idx // self.num_augments\n",
        "        image, label = self.base[base_idx]\n",
        "\n",
        "        if self.heavy_aug and hasattr(self, 'aug_transform'):\n",
        "            if isinstance(image, torch.Tensor):\n",
        "                image = transforms.ToPILImage()(image)\n",
        "            image = self.aug_transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def mixup_cutmix_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Unified criterion for MixUp and CutMix\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "def get_advanced_data_loaders(dataset_name: str, batch_size: int) -> Tuple[DataLoader, DataLoader, tuple, int]:\n",
        "    \"\"\"Advanced data loading with heavy augmentation\"\"\"\n",
        "\n",
        "    if dataset_name == 'FashionMNIST':\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(25),\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.3)),\n",
        "            transforms.RandomResizedCrop(28, scale=(0.6, 1.0)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,)),\n",
        "            transforms.RandomErasing(p=0.7, scale=(0.02, 0.5), ratio=(0.3, 3.3)),\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "        test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "        n_classes = 10\n",
        "        n_channels = 1\n",
        "        img_size = 28\n",
        "\n",
        "    enhanced_train = AdvancedAugmentDataset(train_dataset, 3, heavy_aug=True)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        enhanced_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "        prefetch_factor=3,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size * 2,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "        prefetch_factor=3,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    input_dim = (n_channels, img_size, img_size)\n",
        "    return train_loader, test_loader, input_dim, n_classes\n",
        "\n",
        "# ============================================================================\n",
        "# VISION TRANSFORMER COMPONENTS\n",
        "# ============================================================================\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Self-Attention for Vision Transformer\"\"\"\n",
        "    def __init__(self, dim, num_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with layer normalization\"\"\"\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, dropout=0.1, stochastic_depth=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadAttention(dim, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.stochastic_depth = stochastic_depth\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.stochastic_depth > 0:\n",
        "            if torch.rand(1) < self.stochastic_depth:\n",
        "                return x\n",
        "\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "# ============================================================================\n",
        "# ADVANCED ARCHITECTURE COMPONENTS\n",
        "# ============================================================================\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    \"\"\"Stochastic Depth for regularization\"\"\"\n",
        "    def __init__(self, drop_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        if not self.training or self.drop_rate == 0:\n",
        "            return x + residual\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        keep_prob = 1 - self.drop_rate\n",
        "        mask = torch.rand(batch_size, 1, 1, 1, device=x.device) < keep_prob\n",
        "        return x + residual * mask.float() / keep_prob\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Enhanced Squeeze-and-Excitation Block\"\"\"\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class EfficientBlock(nn.Module):\n",
        "    \"\"\"EfficientNet-inspired block with MBConv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1, expand_ratio=6, se_ratio=0.25):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "        self.use_residual = stride == 1 and in_channels == out_channels\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        if expand_ratio != 1:\n",
        "            layers.extend([\n",
        "                nn.Conv2d(in_channels, hidden_dim, 1, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.SiLU(inplace=True)\n",
        "            ])\n",
        "\n",
        "        layers.extend([\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.SiLU(inplace=True)\n",
        "        ])\n",
        "\n",
        "        if se_ratio > 0:\n",
        "            layers.append(SEBlock(hidden_dim, int(1/se_ratio)))\n",
        "\n",
        "        layers.extend([\n",
        "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        ])\n",
        "\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.stochastic_depth = StochasticDepth(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_residual:\n",
        "            return self.stochastic_depth(x, self.conv(x))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "# ============================================================================\n",
        "# HYBRID CNN-TRANSFORMER ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "class HybridFashionNet(nn.Module):\n",
        "    \"\"\"Hybrid CNN-Transformer for 99% target\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: tuple, hidden_dim: int, output_dim: int, dropout_rate: float = 0.3):\n",
        "        super(HybridFashionNet, self).__init__()\n",
        "\n",
        "        channels, height, width = input_dim\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(channels, 64, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.SiLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.efficient_block1 = EfficientBlock(64, 64, stride=1)\n",
        "        self.efficient_block2 = EfficientBlock(64, 128, stride=2)\n",
        "        self.efficient_block3 = EfficientBlock(128, 128, stride=1)\n",
        "        self.efficient_block4 = EfficientBlock(128, 256, stride=2)\n",
        "        self.efficient_block5 = EfficientBlock(256, 256, stride=1)\n",
        "        self.efficient_block6 = EfficientBlock(256, 512, stride=2)\n",
        "\n",
        "        self.patch_embed = nn.Linear(512, 384)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 16, 384))\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(384, num_heads=8, stochastic_depth=0.1 * i / 4)\n",
        "            for i in range(4)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(384)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.cnn_classifier = nn.Linear(512, hidden_dim // 2)\n",
        "        self.transformer_classifier = nn.Linear(384, hidden_dim // 2)\n",
        "\n",
        "        self.fusion_classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate // 2),\n",
        "\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate // 4),\n",
        "\n",
        "            nn.Linear(hidden_dim // 4, output_dim)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.efficient_block1(x)\n",
        "        x = self.efficient_block2(x)\n",
        "        x = self.efficient_block3(x)\n",
        "        x = self.efficient_block4(x)\n",
        "        x = self.efficient_block5(x)\n",
        "        x = self.efficient_block6(x)\n",
        "\n",
        "        cnn_features = self.global_avg_pool(x).flatten(1)\n",
        "        cnn_out = self.cnn_classifier(cnn_features)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        transformer_input = x.flatten(2).transpose(1, 2)\n",
        "        transformer_input = self.patch_embed(transformer_input)\n",
        "        transformer_input = transformer_input + self.pos_embed\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            transformer_input = block(transformer_input)\n",
        "\n",
        "        transformer_input = self.norm(transformer_input)\n",
        "        transformer_out = self.transformer_classifier(transformer_input.mean(1))\n",
        "\n",
        "        fused = torch.cat([cnn_out, transformer_out], dim=1)\n",
        "        return self.fusion_classifier(fused)\n",
        "\n",
        "class EfficientFashionNet(nn.Module):\n",
        "    \"\"\"EfficientNet-inspired architecture\"\"\"\n",
        "    def __init__(self, input_dim: tuple, hidden_dim: int, output_dim: int, dropout_rate: float = 0.3):\n",
        "        super().__init__()\n",
        "        channels, height, width = input_dim\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(channels, 32, 3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.SiLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            EfficientBlock(32, 64, stride=1, expand_ratio=1),\n",
        "            EfficientBlock(64, 64, stride=1, expand_ratio=4),\n",
        "            EfficientBlock(64, 128, stride=2, expand_ratio=4),\n",
        "            EfficientBlock(128, 128, stride=1, expand_ratio=4),\n",
        "            EfficientBlock(128, 256, stride=2, expand_ratio=4),\n",
        "            EfficientBlock(256, 256, stride=1, expand_ratio=6),\n",
        "            EfficientBlock(256, 512, stride=2, expand_ratio=6),\n",
        "            EfficientBlock(512, 512, stride=1, expand_ratio=6),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, hidden_dim),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.blocks(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ============================================================================\n",
        "# TEST-TIME AUGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_time_augmentation(model: nn.Module, x: torch.Tensor, num_augments: int = 8) -> torch.Tensor:\n",
        "    \"\"\"Test-time augmentation for better accuracy\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with autocast('cuda'):\n",
        "        pred = F.softmax(model(x), dim=1)\n",
        "        predictions.append(pred)\n",
        "\n",
        "    for _ in range(num_augments - 1):\n",
        "        aug_x = x.clone()\n",
        "\n",
        "        if torch.rand(1) < 0.5:\n",
        "            aug_x = torch.flip(aug_x, dims=[3])\n",
        "\n",
        "        if torch.rand(1) < 0.7:\n",
        "            noise = torch.randn_like(aug_x) * 0.02\n",
        "            aug_x = aug_x + noise\n",
        "            aug_x = torch.clamp(aug_x, -1, 1)\n",
        "\n",
        "        with autocast('cuda'):\n",
        "            pred = F.softmax(model(aug_x), dim=1)\n",
        "            predictions.append(pred)\n",
        "\n",
        "    return torch.stack(predictions).mean(0)\n",
        "\n",
        "# ============================================================================\n",
        "# KNOWLEDGE DISTILLATION\n",
        "# ============================================================================\n",
        "\n",
        "class KnowledgeDistillationLoss(nn.Module):\n",
        "    \"\"\"Knowledge distillation loss for ensemble training\"\"\"\n",
        "    def __init__(self, temperature=4.0, alpha=0.3):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n",
        "        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n",
        "\n",
        "        distillation_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)\n",
        "        classification_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        return self.alpha * classification_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "# ============================================================================\n",
        "# ADVANCED TRAINING WITH CHECKPOINTING\n",
        "# ============================================================================\n",
        "\n",
        "def train_advanced_model(model: nn.Module, train_loader: DataLoader, criterion: nn.Module,\n",
        "                        test_loader: DataLoader, optimizer: optim.Optimizer, scheduler,\n",
        "                        num_epochs: int = 200, verbose: bool = True, teacher_model=None,\n",
        "                        model_name: str = \"model\", resume_from_checkpoint: bool = True) -> float:\n",
        "\n",
        "    model = model.to(device)\n",
        "    scaler = GradScaler('cuda')\n",
        "    mixup = MixUp(alpha=0.4)\n",
        "    cutmix = CutMix(alpha=1.0)\n",
        "\n",
        "    # Initialize checkpoint system\n",
        "    checkpoint = ModelCheckpoint()\n",
        "\n",
        "    if teacher_model is not None:\n",
        "        teacher_model.eval()\n",
        "        kd_criterion = KnowledgeDistillationLoss()\n",
        "\n",
        "    # Resume from checkpoint if available\n",
        "    start_epoch = 1\n",
        "    best_accuracy = 0.0\n",
        "    patience_counter = 0\n",
        "    max_patience = 30\n",
        "\n",
        "    if resume_from_checkpoint:\n",
        "        resume_info = checkpoint.get_resume_info()\n",
        "        if resume_info:\n",
        "            print(f\"\\nüîÑ Attempting to resume training for {model_name}...\")\n",
        "            loaded_info = checkpoint.load_checkpoint(\n",
        "                resume_info['checkpoint_path'], model, optimizer, scheduler\n",
        "            )\n",
        "\n",
        "            if loaded_info:\n",
        "                start_epoch = loaded_info['epoch'] + 1\n",
        "                best_accuracy = loaded_info['accuracy']\n",
        "                print(f\"‚úÖ Resumed from epoch {loaded_info['epoch']} with accuracy {best_accuracy:.2f}%\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Failed to load checkpoint, starting from scratch\")\n",
        "        else:\n",
        "            print(f\"‚ÑπÔ∏è No checkpoint found for {model_name}, starting fresh training\")\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
        "            batch_x = batch_x.to(device, non_blocking=True)\n",
        "            batch_y = batch_y.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast('cuda'):\n",
        "                aug_prob = np.random.random()\n",
        "\n",
        "                if epoch > 30 and aug_prob < 0.3:\n",
        "                    mixed_x, y_a, y_b, lam = mixup(batch_x, batch_y)\n",
        "                    outputs = model(mixed_x)\n",
        "                    loss = mixup_cutmix_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "                elif epoch > 30 and aug_prob < 0.6:\n",
        "                    mixed_x, y_a, y_b, lam = cutmix(batch_x, batch_y)\n",
        "                    outputs = model(mixed_x)\n",
        "                    loss = mixup_cutmix_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "                else:\n",
        "                    outputs = model(batch_x)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "\n",
        "                    if teacher_model is not None and epoch > 50:\n",
        "                        with torch.no_grad():\n",
        "                            teacher_outputs = teacher_model(batch_x)\n",
        "                        kd_loss = kd_criterion(outputs, teacher_outputs, batch_y)\n",
        "                        loss = 0.7 * loss + 0.3 * kd_loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if torch.cuda.is_available() and batch_idx % 50 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Evaluate and save checkpoints\n",
        "        if epoch % SAVE_EVERY_N_EPOCHS == 0 or epoch == num_epochs:\n",
        "            test_loss, test_acc = evaluate_advanced_model(model, test_loader, criterion)\n",
        "\n",
        "            if verbose:\n",
        "                current_time = datetime.now().strftime(\"%B %d, %Y at %I:%M:%S %p\")\n",
        "                print(f\"Epoch {epoch}/{num_epochs}: Loss = {avg_loss:.4f} | \"\n",
        "                      f\"Test Acc = {test_acc:.2f}% | LR = {scheduler.get_last_lr()[0]:.6f} | Time: {current_time}\")\n",
        "                print_gpu_utilization()\n",
        "\n",
        "            # Save checkpoint if it's the best so far\n",
        "            if test_acc > best_accuracy:\n",
        "                best_accuracy = test_acc\n",
        "                patience_counter = 0\n",
        "\n",
        "                # Save the best model\n",
        "                checkpoint.save_checkpoint(\n",
        "                    model, optimizer, scheduler, epoch, test_acc, avg_loss,\n",
        "                    model_name=model_name,\n",
        "                    additional_info={\n",
        "                        'training_config': {\n",
        "                            'hidden_dim': model.fusion_classifier[0].in_features if hasattr(model, 'fusion_classifier') else 'unknown',\n",
        "                            'dropout_rate': 'variable',\n",
        "                            'batch_size': train_loader.batch_size,\n",
        "                            'learning_rate': scheduler.get_last_lr()[0],\n",
        "                        }\n",
        "                    }\n",
        "                )\n",
        "                print(f\"üéØ New best accuracy: {test_acc:.2f}% (improved by {test_acc - (best_accuracy if best_accuracy != test_acc else 0):.2f}%)\")\n",
        "            else:\n",
        "                patience_counter += SAVE_EVERY_N_EPOCHS\n",
        "\n",
        "            # Also save periodic checkpoint regardless of performance\n",
        "            if epoch % (SAVE_EVERY_N_EPOCHS * 4) == 0:\n",
        "                checkpoint.save_checkpoint(\n",
        "                    model, optimizer, scheduler, epoch, test_acc, avg_loss,\n",
        "                    model_name=f\"{model_name}_periodic\",\n",
        "                    additional_info={'checkpoint_type': 'periodic'}\n",
        "                )\n",
        "\n",
        "            if patience_counter >= max_patience:\n",
        "                print(f\"‚èπÔ∏è Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # Final checkpoint save\n",
        "    if epoch == num_epochs or patience_counter >= max_patience:\n",
        "        final_test_loss, final_test_acc = evaluate_advanced_model(model, test_loader, criterion)\n",
        "        checkpoint.save_checkpoint(\n",
        "            model, optimizer, scheduler, epoch, final_test_acc, final_test_loss,\n",
        "            model_name=f\"{model_name}_final\",\n",
        "            additional_info={'checkpoint_type': 'final'}\n",
        "        )\n",
        "\n",
        "    return best_accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_advanced_model(model: nn.Module, test_loader: DataLoader, criterion: nn.Module,\n",
        "                           use_tta: bool = False) -> Tuple[float, float]:\n",
        "    \"\"\"Advanced evaluation with optional TTA\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x = batch_x.to(device, non_blocking=True)\n",
        "        batch_y = batch_y.to(device, non_blocking=True)\n",
        "\n",
        "        if use_tta:\n",
        "            outputs = test_time_augmentation(model, batch_x, TTA_AUGMENTS)\n",
        "        else:\n",
        "            with autocast('cuda'):\n",
        "                outputs = F.softmax(model(batch_x), dim=1)\n",
        "\n",
        "        if use_tta:\n",
        "            raw_outputs = torch.log(outputs + 1e-8)\n",
        "        else:\n",
        "            raw_outputs = model(batch_x)\n",
        "\n",
        "        loss = criterion(raw_outputs, batch_y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == batch_y).sum().item()\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ============================================================================\n",
        "# ADVANCED ENSEMBLE SYSTEM WITH CHECKPOINTING\n",
        "# ============================================================================\n",
        "\n",
        "def train_diverse_ensemble(input_dim, hidden_dim, n_classes, train_loader, test_loader,\n",
        "                          ensemble_size=7, resume_training=True) -> Tuple[List[nn.Module], List[float]]:\n",
        "    \"\"\"Train diverse ensemble with checkpoint support\"\"\"\n",
        "\n",
        "    models = []\n",
        "    best_accuracies = []\n",
        "    architectures = []\n",
        "\n",
        "    model_configs = [\n",
        "        (HybridFashionNet, \"Hybrid_CNN_Transformer\"),\n",
        "        (EfficientFashionNet, \"EfficientNet_Style\"),\n",
        "        (lambda input_dim, hidden_dim, n_classes, dropout_rate:\n",
        "         HybridFashionNet(input_dim, hidden_dim, n_classes, dropout_rate), \"Hybrid_Variant_1\"),\n",
        "        (lambda input_dim, hidden_dim, n_classes, dropout_rate:\n",
        "         EfficientFashionNet(input_dim, hidden_dim + 256, n_classes, dropout_rate), \"Large_EfficientNet\"),\n",
        "        (lambda input_dim, hidden_dim, n_classes, dropout_rate:\n",
        "         HybridFashionNet(input_dim, hidden_dim + 512, n_classes, dropout_rate * 0.8), \"Large_Hybrid\"),\n",
        "        (lambda input_dim, hidden_dim, n_classes, dropout_rate:\n",
        "         EfficientFashionNet(input_dim, hidden_dim, n_classes, dropout_rate * 1.2), \"Regularized_EfficientNet\"),\n",
        "        (lambda input_dim, hidden_dim, n_classes, dropout_rate:\n",
        "         HybridFashionNet(input_dim, hidden_dim, n_classes, dropout_rate * 1.1), \"Regularized_Hybrid\"),\n",
        "    ]\n",
        "\n",
        "    for i in range(min(ensemble_size, len(model_configs))):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Training Ensemble Model {i+1}/{ensemble_size}: {model_configs[i][1]}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        model_class = model_configs[i][0]\n",
        "        model_name = f\"ensemble_{i+1}_{model_configs[i][1]}\"\n",
        "        model_hidden = hidden_dim + (i * 128)\n",
        "        model_dropout = DROPOUT_RATE + i * 0.02\n",
        "\n",
        "        model = model_class(input_dim, model_hidden, n_classes, model_dropout)\n",
        "        model = model.to(device)\n",
        "        architectures.append(model_configs[i][1])\n",
        "\n",
        "        if i % 2 == 0:\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE * (0.8 + i*0.05),\n",
        "                                   weight_decay=weight_decay_ * (1 + i*0.1), betas=(0.9, 0.999))\n",
        "        else:\n",
        "            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE * (0.9 + i*0.05),\n",
        "                                  weight_decay=weight_decay_ * (1 + i*0.1))\n",
        "\n",
        "        if i % 3 == 0:\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                optimizer, T_0=EPOCHS_PER_DATASET//4, eta_min=1e-7)\n",
        "        elif i % 3 == 1:\n",
        "            scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "                optimizer, max_lr=LEARNING_RATE * 2, epochs=EPOCHS_PER_DATASET,\n",
        "                steps_per_epoch=len(train_loader))\n",
        "        else:\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "                optimizer, T_max=EPOCHS_PER_DATASET, eta_min=1e-7)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1 + i*0.02)\n",
        "\n",
        "        teacher_model = models[-1] if len(models) > 0 and best_accuracies[-1] > 94.0 else None\n",
        "\n",
        "        best_acc = train_advanced_model(\n",
        "            model, train_loader, criterion, test_loader, optimizer, scheduler,\n",
        "            num_epochs=EPOCHS_PER_DATASET, teacher_model=teacher_model,\n",
        "            model_name=model_name, resume_from_checkpoint=resume_training\n",
        "        )\n",
        "\n",
        "        models.append(model)\n",
        "        best_accuracies.append(best_acc)\n",
        "\n",
        "        print(f\"‚úÖ Model {i+1} ({model_configs[i][1]}) Best Accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return models, best_accuracies\n",
        "\n",
        "@torch.no_grad()\n",
        "def advanced_ensemble_predict(models: List[nn.Module], test_loader: DataLoader,\n",
        "                            use_tta: bool = True) -> float:\n",
        "    \"\"\"Advanced ensemble prediction with TTA and weighted voting\"\"\"\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x = batch_x.to(device, non_blocking=True)\n",
        "        batch_y = batch_y.to(device, non_blocking=True)\n",
        "\n",
        "        ensemble_outputs = []\n",
        "        for model in models:\n",
        "            if use_tta:\n",
        "                outputs = test_time_augmentation(model, batch_x, TTA_AUGMENTS)\n",
        "            else:\n",
        "                with autocast('cuda'):\n",
        "                    outputs = F.softmax(model(batch_x), dim=1)\n",
        "            ensemble_outputs.append(outputs)\n",
        "\n",
        "        avg_outputs = torch.stack(ensemble_outputs).mean(0)\n",
        "        preds = avg_outputs.argmax(dim=1)\n",
        "\n",
        "        correct += (preds == batch_y).sum().item()\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXPERIMENT PIPELINE WITH CHECKPOINTING\n",
        "# ============================================================================\n",
        "\n",
        "def run_advanced_experiments(resume_training=True):\n",
        "    \"\"\"Run advanced experiments with checkpoint support\"\"\"\n",
        "\n",
        "    for dataset_name in DATASET_NAMES:\n",
        "        print(f\"\\n{'='*90}\")\n",
        "        print(f\"RUNNING ADVANCED SOTA EXPERIMENTS ON {dataset_name}\")\n",
        "        print(f\"TARGET: 99% ACCURACY WITH ADVANCED TECHNIQUES + CHECKPOINTING\")\n",
        "        print(f\"{'='*90}\")\n",
        "\n",
        "        train_loader, test_loader, input_dim, n_classes = get_advanced_data_loaders(dataset_name, BATCH_SIZE)\n",
        "        print(f\"Dataset: {dataset_name} | Input: {input_dim} | Classes: {n_classes}\")\n",
        "        print(f\"Train: {len(train_loader.dataset)} | Test: {len(test_loader.dataset)}\")\n",
        "        print(f\"Advanced Augmentation: Enabled | TTA: {TTA_AUGMENTS} augments\")\n",
        "        print(f\"Checkpoint Directory: {CHECKPOINT_DIR}\")\n",
        "        print(f\"Resume Training: {'Enabled' if resume_training else 'Disabled'}\")\n",
        "        print_gpu_utilization()\n",
        "\n",
        "        # List existing checkpoints\n",
        "        checkpoint_manager = ModelCheckpoint()\n",
        "        checkpoint_manager.list_checkpoints()\n",
        "\n",
        "        models, individual_accuracies = train_diverse_ensemble(\n",
        "            input_dim, HIDDEN_DIM, n_classes, train_loader, test_loader,\n",
        "            ENSEMBLE_SIZE, resume_training=resume_training\n",
        "        )\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"INDIVIDUAL MODEL RESULTS\")\n",
        "        print(f\"{'='*70}\")\n",
        "        for i, acc in enumerate(individual_accuracies):\n",
        "            print(f\"Model {i+1}: {acc:.2f}%\")\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ENSEMBLE RESULTS WITH TTA\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        ensemble_acc_no_tta = advanced_ensemble_predict(models, test_loader, use_tta=False)\n",
        "        print(f\"Ensemble Accuracy (no TTA): {ensemble_acc_no_tta:.2f}%\")\n",
        "\n",
        "        ensemble_acc_tta = advanced_ensemble_predict(models, test_loader, use_tta=True)\n",
        "        print(f\"Ensemble Accuracy (with TTA): {ensemble_acc_tta:.2f}%\")\n",
        "\n",
        "        final_accuracy = max(max(individual_accuracies), ensemble_acc_no_tta, ensemble_acc_tta)\n",
        "\n",
        "        # Save ensemble results\n",
        "        ensemble_checkpoint = ModelCheckpoint()\n",
        "        ensemble_results = {\n",
        "            'individual_accuracies': individual_accuracies,\n",
        "            'ensemble_no_tta': ensemble_acc_no_tta,\n",
        "            'ensemble_with_tta': ensemble_acc_tta,\n",
        "            'final_accuracy': final_accuracy,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'experiment_config': {\n",
        "                'dataset': dataset_name,\n",
        "                'ensemble_size': len(models),\n",
        "                'epochs': EPOCHS_PER_DATASET,\n",
        "                'batch_size': BATCH_SIZE,\n",
        "                'hidden_dim': HIDDEN_DIM,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        results_path = Path(CHECKPOINT_DIR) / \"ensemble_results.json\"\n",
        "        with open(results_path, 'w') as f:\n",
        "            json.dump(ensemble_results, f, indent=2)\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"FINAL ADVANCED SOTA RESULTS\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Best Individual Model: {max(individual_accuracies):.2f}%\")\n",
        "        print(f\"Best Ensemble (no TTA): {ensemble_acc_no_tta:.2f}%\")\n",
        "        print(f\"Best Ensemble (with TTA): {ensemble_acc_tta:.2f}%\")\n",
        "        print(f\"FINAL BEST ACCURACY: {final_accuracy:.2f}%\")\n",
        "        print(f\"Results saved to: {results_path}\")\n",
        "\n",
        "        if final_accuracy >= 99.0:\n",
        "            print(f\"üéâüéâ OUTSTANDING! {final_accuracy:.2f}% ‚â• 99% TARGET ACHIEVED!\")\n",
        "        elif final_accuracy >= 97.0:\n",
        "            print(f\"üéâ EXCELLENT! {final_accuracy:.2f}% ‚â• 97%\")\n",
        "        elif final_accuracy >= 95.0:\n",
        "            print(f\"‚úÖ VERY GOOD! {final_accuracy:.2f}% ‚â• 95%\")\n",
        "        elif final_accuracy >= 92.0:\n",
        "            print(f\"üü° GOOD! {final_accuracy:.2f}% ‚â• 92%\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è NEEDS IMPROVEMENT: {final_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    SEED_ = 42\n",
        "    print(\"=\"*90)\n",
        "    print(\"ADVANCED SOTA FASHIONMNIST WITH CHECKPOINT SYSTEM\")\n",
        "    print(\"FEATURES: Model Saving/Loading, Resume Training, Hybrid Architecture\")\n",
        "    print(\"=\"*90)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Ensemble Size: {ENSEMBLE_SIZE}\")\n",
        "    print(f\"Epochs per Model: {EPOCHS_PER_DATASET}\")\n",
        "    print(f\"Hidden Dim: {HIDDEN_DIM}\")\n",
        "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "    print(f\"Checkpoint Directory: {CHECKPOINT_DIR}\")\n",
        "    print(f\"Save Top K Models: {SAVE_TOP_K}\")\n",
        "    print(f\"Save Every N Epochs: {SAVE_EVERY_N_EPOCHS}\")\n",
        "\n",
        "    torch.manual_seed(SEED_)\n",
        "    np.random.seed(SEED_)\n",
        "    random.seed(SEED_)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "\n",
        "    # Option to resume training or start fresh\n",
        "    resume_choice = input(\"\\nResume training from checkpoints? (y/n, default=y): \").strip().lower()\n",
        "    resume_training = resume_choice != 'n'\n",
        "\n",
        "    run_advanced_experiments(resume_training=resume_training)\n",
        "    print(\"\\nüéâ ADVANCED SOTA EXPERIMENTS WITH CHECKPOINTING COMPLETED!\")\n",
        "\n",
        "    # Final checkpoint summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"CHECKPOINT SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    checkpoint_manager = ModelCheckpoint()\n",
        "    checkpoint_manager.list_checkpoints()\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()\n"
      ],
      "metadata": {
        "id": "3Jykg_CjPort"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Tuple, Dict, Any, Optional, List\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torch.amp import autocast, GradScaler  # Updated import\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import copy\n",
        "from collections import OrderedDict\n",
        "\n",
        "# ============================================================================\n",
        "# GPU DEVICE SETUP FOR T4\n",
        "# ============================================================================\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory Used: {torch.cuda.memory_allocated(0) / 1024**3:.1f} GB\")\n",
        "        print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1024**3:.1f} GB\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCED CONFIGURATION FOR 99% TARGET\n",
        "# ============================================================================\n",
        "\n",
        "DATASET_NAMES = ['FashionMNIST']\n",
        "HIDDEN_DIM: int = 3072  # Increased capacity\n",
        "LEARNING_RATE: float = 8e-4  # Slightly reduced for stability\n",
        "weight_decay_ = 2e-4  # Increased regularization\n",
        "DROPOUT_RATE: float = 0.3  # Reduced for better capacity\n",
        "EPOCHS_PER_DATASET: int = 200  # Extended training\n",
        "BATCH_SIZE: int = 512  # Reduced for more stable training\n",
        "ENSEMBLE_SIZE: int = 7  # Larger ensemble\n",
        "TTA_AUGMENTS: int = 8  # Test-time augmentation\n",
        "\n",
        "# ============================================================================\n",
        "# ADVANCED DATA AUGMENTATION STRATEGIES\n",
        "# ============================================================================\n",
        "\n",
        "class CutMix(object):\n",
        "    \"\"\"CutMix augmentation\"\"\"\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        if self.alpha <= 0:\n",
        "            return x, y, y, 1.0\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        lam = np.random.beta(self.alpha, self.alpha)\n",
        "        index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "        bbx1, bby1, bbx2, bby2 = self._rand_bbox(x.size(), lam)\n",
        "        x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "        # Adjust lambda to match pixel ratio\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
        "        y_a, y_b = y, y[index]\n",
        "        return x, y_a, y_b, lam\n",
        "\n",
        "    def _rand_bbox(self, size, lam):\n",
        "        W = size[2]\n",
        "        H = size[3]\n",
        "        cut_rat = np.sqrt(1. - lam)\n",
        "        cut_w = np.int32(W * cut_rat)\n",
        "        cut_h = np.int32(H * cut_rat)\n",
        "\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "class MixUp(object):\n",
        "    \"\"\"Enhanced MixUp with adaptive alpha\"\"\"\n",
        "    def __init__(self, alpha=0.4):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        if self.alpha > 0:\n",
        "            lam = np.random.beta(self.alpha, self.alpha)\n",
        "        else:\n",
        "            lam = 1\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "        y_a, y_b = y, y[index]\n",
        "        return mixed_x, y_a, y_b, lam\n",
        "\n",
        "class AdvancedAugmentDataset(Dataset):\n",
        "    \"\"\"Advanced dataset with sophisticated augmentations\"\"\"\n",
        "    def __init__(self, base_dataset, num_augments=3, heavy_aug=True):\n",
        "        self.base = base_dataset\n",
        "        self.num_augments = num_augments\n",
        "        self.heavy_aug = heavy_aug\n",
        "\n",
        "        # Heavy augmentation for training\n",
        "        if heavy_aug:\n",
        "            self.aug_transform = transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomRotation(20),\n",
        "                transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.8, 1.2)),\n",
        "                transforms.RandomResizedCrop(28, scale=(0.7, 1.0)),\n",
        "                transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "                transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,), (0.5,)),\n",
        "                transforms.RandomErasing(p=0.6, scale=(0.02, 0.4), ratio=(0.3, 3.3)),\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base) * self.num_augments\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_idx = idx // self.num_augments\n",
        "        image, label = self.base[base_idx]\n",
        "\n",
        "        if self.heavy_aug and hasattr(self, 'aug_transform'):\n",
        "            # Convert tensor back to PIL for additional augmentation\n",
        "            if isinstance(image, torch.Tensor):\n",
        "                image = transforms.ToPILImage()(image)\n",
        "            image = self.aug_transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def mixup_cutmix_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Unified criterion for MixUp and CutMix\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "def get_advanced_data_loaders(dataset_name: str, batch_size: int) -> Tuple[DataLoader, DataLoader, tuple, int]:\n",
        "    \"\"\"Advanced data loading with heavy augmentation\"\"\"\n",
        "\n",
        "    if dataset_name == 'FashionMNIST':\n",
        "        # Sophisticated training augmentation\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(25),\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.3)),\n",
        "            transforms.RandomResizedCrop(28, scale=(0.6, 1.0)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,)),\n",
        "            transforms.RandomErasing(p=0.7, scale=(0.02, 0.5), ratio=(0.3, 3.3)),\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "        test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "        n_classes = 10\n",
        "        n_channels = 1\n",
        "        img_size = 28\n",
        "\n",
        "    # Heavy augmentation with 3x data multiplication\n",
        "    enhanced_train = AdvancedAugmentDataset(train_dataset, 3, heavy_aug=True)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        enhanced_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,  # Increased workers\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "        prefetch_factor=3,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size * 2,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "        prefetch_factor=3,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    input_dim = (n_channels, img_size, img_size)\n",
        "    return train_loader, test_loader, input_dim, n_classes\n",
        "\n",
        "# ============================================================================\n",
        "# VISION TRANSFORMER COMPONENTS\n",
        "# ============================================================================\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Self-Attention for Vision Transformer\"\"\"\n",
        "    def __init__(self, dim, num_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with layer normalization\"\"\"\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, dropout=0.1, stochastic_depth=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadAttention(dim, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.stochastic_depth = stochastic_depth\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Stochastic depth for regularization\n",
        "        if self.training and self.stochastic_depth > 0:\n",
        "            if torch.rand(1) < self.stochastic_depth:\n",
        "                return x\n",
        "\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "# ============================================================================\n",
        "# ADVANCED ARCHITECTURE COMPONENTS\n",
        "# ============================================================================\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    \"\"\"Stochastic Depth for regularization\"\"\"\n",
        "    def __init__(self, drop_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        if not self.training or self.drop_rate == 0:\n",
        "            return x + residual\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        keep_prob = 1 - self.drop_rate\n",
        "        mask = torch.rand(batch_size, 1, 1, 1, device=x.device) < keep_prob\n",
        "        return x + residual * mask.float() / keep_prob\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Enhanced Squeeze-and-Excitation Block\"\"\"\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class EfficientBlock(nn.Module):\n",
        "    \"\"\"EfficientNet-inspired block with MBConv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1, expand_ratio=6, se_ratio=0.25):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "        self.use_residual = stride == 1 and in_channels == out_channels\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # Pointwise expansion\n",
        "        if expand_ratio != 1:\n",
        "            layers.extend([\n",
        "                nn.Conv2d(in_channels, hidden_dim, 1, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.SiLU(inplace=True)\n",
        "            ])\n",
        "\n",
        "        # Depthwise convolution\n",
        "        layers.extend([\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.SiLU(inplace=True)\n",
        "        ])\n",
        "\n",
        "        # SE block\n",
        "        if se_ratio > 0:\n",
        "            layers.append(SEBlock(hidden_dim, int(1/se_ratio)))\n",
        "\n",
        "        # Pointwise compression\n",
        "        layers.extend([\n",
        "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        ])\n",
        "\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.stochastic_depth = StochasticDepth(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_residual:\n",
        "            return self.stochastic_depth(x, self.conv(x))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Enhanced Residual Block with advanced features\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1, use_se=True, drop_path=0.1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.se = SEBlock(out_channels) if use_se else None\n",
        "        self.dropout = nn.Dropout2d(0.1)\n",
        "        self.stochastic_depth = StochasticDepth(drop_path)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.shortcut(x)\n",
        "\n",
        "        out = F.silu(self.bn1(self.conv1(x)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.se:\n",
        "            out = self.se(out)\n",
        "\n",
        "        return self.stochastic_depth(residual, out)\n",
        "\n",
        "# ============================================================================\n",
        "# HYBRID CNN-TRANSFORMER ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "class HybridFashionNet(nn.Module):\n",
        "    \"\"\"Hybrid CNN-Transformer for 99% target\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: tuple, hidden_dim: int, output_dim: int, dropout_rate: float = 0.3):\n",
        "        super(HybridFashionNet, self).__init__()\n",
        "\n",
        "        channels, height, width = input_dim\n",
        "\n",
        "        # CNN Feature Extraction\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(channels, 64, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.SiLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # EfficientNet-style blocks\n",
        "        self.efficient_block1 = EfficientBlock(64, 64, stride=1)\n",
        "        self.efficient_block2 = EfficientBlock(64, 128, stride=2)\n",
        "        self.efficient_block3 = EfficientBlock(128, 128, stride=1)\n",
        "        self.efficient_block4 = EfficientBlock(128, 256, stride=2)\n",
        "        self.efficient_block5 = EfficientBlock(256, 256, stride=1)\n",
        "        self.efficient_block6 = EfficientBlock(256, 512, stride=2)\n",
        "\n",
        "        # Transformer components\n",
        "        self.patch_embed = nn.Linear(512, 384)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 16, 384))  # 4x4 patches after CNN\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(384, num_heads=8, stochastic_depth=0.1 * i / 4)\n",
        "            for i in range(4)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(384)\n",
        "\n",
        "        # Advanced classifier with multiple paths\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.cnn_classifier = nn.Linear(512, hidden_dim // 2)\n",
        "\n",
        "        self.transformer_classifier = nn.Linear(384, hidden_dim // 2)\n",
        "\n",
        "        self.fusion_classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate // 2),\n",
        "\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate // 4),\n",
        "\n",
        "            nn.Linear(hidden_dim // 4, output_dim)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Advanced weight initialization\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Initialize positional embeddings\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN path\n",
        "        x = self.stem(x)\n",
        "        x = self.efficient_block1(x)\n",
        "        x = self.efficient_block2(x)\n",
        "        x = self.efficient_block3(x)\n",
        "        x = self.efficient_block4(x)\n",
        "        x = self.efficient_block5(x)\n",
        "        x = self.efficient_block6(x)\n",
        "\n",
        "        # Split for dual processing\n",
        "        cnn_features = self.global_avg_pool(x).flatten(1)\n",
        "        cnn_out = self.cnn_classifier(cnn_features)\n",
        "\n",
        "        # Transformer path\n",
        "        B, C, H, W = x.shape\n",
        "        transformer_input = x.flatten(2).transpose(1, 2)  # B, HW, C\n",
        "        transformer_input = self.patch_embed(transformer_input)\n",
        "        transformer_input = transformer_input + self.pos_embed\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            transformer_input = block(transformer_input)\n",
        "\n",
        "        transformer_input = self.norm(transformer_input)\n",
        "        transformer_out = self.transformer_classifier(transformer_input.mean(1))\n",
        "\n",
        "        # Fusion\n",
        "        fused = torch.cat([cnn_out, transformer_out], dim=1)\n",
        "        return self.fusion_classifier(fused)\n",
        "\n",
        "# ============================================================================\n",
        "# ADVANCED ENSEMBLE ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "class EfficientFashionNet(nn.Module):\n",
        "    \"\"\"EfficientNet-inspired architecture\"\"\"\n",
        "    def __init__(self, input_dim: tuple, hidden_dim: int, output_dim: int, dropout_rate: float = 0.3):\n",
        "        super().__init__()\n",
        "        channels, height, width = input_dim\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(channels, 32, 3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.SiLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            EfficientBlock(32, 64, stride=1, expand_ratio=1),\n",
        "            EfficientBlock(64, 64, stride=1, expand_ratio=4),\n",
        "            EfficientBlock(64, 128, stride=2, expand_ratio=4),\n",
        "            EfficientBlock(128, 128, stride=1, expand_ratio=4),\n",
        "            EfficientBlock(128, 256, stride=2, expand_ratio=4),\n",
        "            EfficientBlock(256, 256, stride=1, expand_ratio=6),\n",
        "            EfficientBlock(256, 512, stride=2, expand_ratio=6),\n",
        "            EfficientBlock(512, 512, stride=1, expand_ratio=6),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, hidden_dim),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.blocks(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ============================================================================\n",
        "# TEST-TIME AUGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_time_augmentation(model: nn.Module, x: torch.Tensor, num_augments: int = 8) -> torch.Tensor:\n",
        "    \"\"\"Test-time augmentation for better accuracy\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    # Original prediction\n",
        "    with autocast('cuda'):\n",
        "        pred = F.softmax(model(x), dim=1)\n",
        "        predictions.append(pred)\n",
        "\n",
        "    # Augmented predictions\n",
        "    for _ in range(num_augments - 1):\n",
        "        # Random augmentations\n",
        "        aug_x = x.clone()\n",
        "\n",
        "        # Random horizontal flip\n",
        "        if torch.rand(1) < 0.5:\n",
        "            aug_x = torch.flip(aug_x, dims=[3])\n",
        "\n",
        "        # Small random rotation (simulated with small translations)\n",
        "        if torch.rand(1) < 0.7:\n",
        "            # Small random noise\n",
        "            noise = torch.randn_like(aug_x) * 0.02\n",
        "            aug_x = aug_x + noise\n",
        "            aug_x = torch.clamp(aug_x, -1, 1)\n",
        "\n",
        "        with autocast('cuda'):\n",
        "            pred = F.softmax(model(aug_x), dim=1)\n",
        "            predictions.append(pred)\n",
        "\n",
        "    return torch.stack(predictions).mean(0)\n",
        "\n",
        "# ============================================================================\n",
        "# ADVANCED TRAINING WITH KNOWLEDGE DISTILLATION\n",
        "# ============================================================================\n",
        "\n",
        "class KnowledgeDistillationLoss(nn.Module):\n",
        "    \"\"\"Knowledge distillation loss for ensemble training\"\"\"\n",
        "    def __init__(self, temperature=4.0, alpha=0.3):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # Soft targets from teacher\n",
        "        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n",
        "        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n",
        "\n",
        "        distillation_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)\n",
        "        classification_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        return self.alpha * classification_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "def train_advanced_model(model: nn.Module, train_loader: DataLoader, criterion: nn.Module,\n",
        "                        test_loader: DataLoader, optimizer: optim.Optimizer, scheduler,\n",
        "                        num_epochs: int = 200, verbose: bool = True, teacher_model=None) -> float:\n",
        "\n",
        "    model = model.to(device)\n",
        "    scaler = GradScaler('cuda')\n",
        "    mixup = MixUp(alpha=0.4)\n",
        "    cutmix = CutMix(alpha=1.0)\n",
        "\n",
        "    if teacher_model is not None:\n",
        "        teacher_model.eval()\n",
        "        kd_criterion = KnowledgeDistillationLoss()\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    patience_counter = 0\n",
        "    max_patience = 30\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
        "            batch_x = batch_x.to(device, non_blocking=True)\n",
        "            batch_y = batch_y.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast('cuda'):\n",
        "                # Decide augmentation strategy\n",
        "                aug_prob = np.random.random()\n",
        "\n",
        "                if epoch > 30 and aug_prob < 0.3:\n",
        "                    # MixUp\n",
        "                    mixed_x, y_a, y_b, lam = mixup(batch_x, batch_y)\n",
        "                    outputs = model(mixed_x)\n",
        "                    loss = mixup_cutmix_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "                elif epoch > 30 and aug_prob < 0.6:\n",
        "                    # CutMix\n",
        "                    mixed_x, y_a, y_b, lam = cutmix(batch_x, batch_y)\n",
        "                    outputs = model(mixed_x)\n",
        "                    loss = mixup_cutmix_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "                else:\n",
        "                    # Standard training\n",
        "                    outputs = model(batch_x)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "\n",
        "                    # Knowledge distillation if teacher available\n",
        "                    if teacher_model is not None and epoch > 50:\n",
        "                        with torch.no_grad():\n",
        "                            teacher_outputs = teacher_model(batch_x)\n",
        "                        kd_loss = kd_criterion(outputs, teacher_outputs, batch_y)\n",
        "                        loss = 0.7 * loss + 0.3 * kd_loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if torch.cuda.is_available() and batch_idx % 50 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        if epoch % 5 == 0:  # Less frequent evaluation to save time\n",
        "            test_loss, test_acc = evaluate_advanced_model(model, test_loader, criterion)\n",
        "\n",
        "            if verbose:\n",
        "                current_time = datetime.now().strftime(\"%B %d, %Y at %I:%M:%S %p\")\n",
        "                print(f\"Epoch {epoch}/{num_epochs}: Loss = {avg_loss:.4f} | \"\n",
        "                      f\"Test Acc = {test_acc:.2f}% | LR = {scheduler.get_last_lr()[0]:.6f} | Time: {current_time}\")\n",
        "                print_gpu_utilization()\n",
        "\n",
        "            if test_acc > best_accuracy:\n",
        "                best_accuracy = test_acc\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 5  # Increment by evaluation interval\n",
        "\n",
        "            if patience_counter >= max_patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    return best_accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_advanced_model(model: nn.Module, test_loader: DataLoader, criterion: nn.Module,\n",
        "                           use_tta: bool = True) -> Tuple[float, float]:\n",
        "    \"\"\"Advanced evaluation with optional TTA\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x = batch_x.to(device, non_blocking=True)\n",
        "        batch_y = batch_y.to(device, non_blocking=True)\n",
        "\n",
        "        if use_tta:\n",
        "            outputs = test_time_augmentation(model, batch_x, TTA_AUGMENTS)\n",
        "        else:\n",
        "            with autocast('cuda'):\n",
        "                outputs = F.softmax(model(batch_x), dim=1)\n",
        "\n",
        "        # Calculate loss using model's raw logits for TTA compatibility\n",
        "        if use_tta:\n",
        "            # Convert back to logits for loss calculation\n",
        "            raw_outputs = torch.log(outputs + 1e-8)\n",
        "        else:\n",
        "            raw_outputs = model(batch_x)\n",
        "\n",
        "        loss = criterion(raw_outputs, batch_y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == batch_y).sum().item()\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ============================================================================\n",
        "# ADVANCED ENSEMBLE SYSTEM\n",
        "# ============================================================================\n",
        "\n",
        "def train_diverse_ensemble(input_dim, hidden_dim, n_classes, train_loader, test_loader,\n",
        "                          ensemble_size=7) -> Tuple[List[nn.Module], List[float]]:\n",
        "    \"\"\"Train diverse ensemble with different architectures\"\"\"\n",
        "\n",
        "    models = []\n",
        "    best_accuracies = []\n",
        "    architectures = []\n",
        "\n",
        "    # Define diverse architectures\n",
        "    model_configs = [\n",
        "        (HybridFashionNet, \"Hybrid CNN-Transformer\"),\n",
        "        (EfficientFashionNet, \"EfficientNet-style\"),\n",
        "        (lambda input_dim, hidden_dim, n_classes, dropout_rate:\n",
        "         HybridFashionNet(input_dim, hidden_dim, n_classes, dropout_rate), \"Hybrid Variant 1\"),\n",
        "        (lambda input_dim, hidden_dim, n_classes, dropout_rate:\n",
        "         EfficientFashionNet(input_dim, hidden_dim + 256, n_classes, dropout_rate), \"Large EfficientNet\"),\n",
        "        (lambda input_dim, hidden_dim, n_classes, dropout_rate:\n",
        "         HybridFashionNet(input_dim, hidden_dim + 512, n_classes, dropout_rate * 0.8), \"Large Hybrid\"),\n",
        "        (lambda input_dim, hidden_dim, n_classes, dropout_rate:\n",
        "         EfficientFashionNet(input_dim, hidden_dim, n_classes, dropout_rate * 1.2), \"Regularized EfficientNet\"),\n",
        "        (lambda input_dim, hidden_dim, n_classes, dropout_rate:\n",
        "         HybridFashionNet(input_dim, hidden_dim, n_classes, dropout_rate * 1.1), \"Regularized Hybrid\"),\n",
        "    ]\n",
        "\n",
        "    for i in range(min(ensemble_size, len(model_configs))):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Training Ensemble Model {i+1}/{ensemble_size}: {model_configs[i][1]}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        model_class = model_configs[i][0]\n",
        "        model_hidden = hidden_dim + (i * 128)\n",
        "        model_dropout = DROPOUT_RATE + i * 0.02\n",
        "\n",
        "        model = model_class(input_dim, model_hidden, n_classes, model_dropout)\n",
        "        model = model.to(device)\n",
        "        architectures.append(model_configs[i][1])\n",
        "\n",
        "        # Varied optimizers and schedules\n",
        "        if i % 2 == 0:\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE * (0.8 + i*0.05),\n",
        "                                   weight_decay=weight_decay_ * (1 + i*0.1), betas=(0.9, 0.999))\n",
        "        else:\n",
        "            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE * (0.9 + i*0.05),\n",
        "                                  weight_decay=weight_decay_ * (1 + i*0.1))\n",
        "\n",
        "        # Different schedulers for diversity\n",
        "        if i % 3 == 0:\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                optimizer, T_0=EPOCHS_PER_DATASET//4, eta_min=1e-7)\n",
        "        elif i % 3 == 1:\n",
        "            scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "                optimizer, max_lr=LEARNING_RATE * 2, epochs=EPOCHS_PER_DATASET,\n",
        "                steps_per_epoch=len(train_loader))\n",
        "        else:\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "                optimizer, T_max=EPOCHS_PER_DATASET, eta_min=1e-7)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1 + i*0.02)\n",
        "\n",
        "        # Use previous best model as teacher for knowledge distillation\n",
        "        teacher_model = models[-1] if len(models) > 0 and best_accuracies[-1] > 94.0 else None\n",
        "\n",
        "        best_acc = train_advanced_model(\n",
        "            model, train_loader, criterion, test_loader, optimizer, scheduler,\n",
        "            num_epochs=EPOCHS_PER_DATASET, teacher_model=teacher_model\n",
        "        )\n",
        "\n",
        "        models.append(model)\n",
        "        best_accuracies.append(best_acc)\n",
        "\n",
        "        print(f\"Model {i+1} ({model_configs[i][1]}) Best Accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return models, best_accuracies\n",
        "\n",
        "@torch.no_grad()\n",
        "def advanced_ensemble_predict(models: List[nn.Module], test_loader: DataLoader,\n",
        "                            use_tta: bool = True) -> float:\n",
        "    \"\"\"Advanced ensemble prediction with TTA and weighted voting\"\"\"\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x = batch_x.to(device, non_blocking=True)\n",
        "        batch_y = batch_y.to(device, non_blocking=True)\n",
        "\n",
        "        ensemble_outputs = []\n",
        "        for model in models:\n",
        "            if use_tta:\n",
        "                outputs = test_time_augmentation(model, batch_x, TTA_AUGMENTS)\n",
        "            else:\n",
        "                with autocast('cuda'):\n",
        "                    outputs = F.softmax(model(batch_x), dim=1)\n",
        "            ensemble_outputs.append(outputs)\n",
        "\n",
        "        # Weighted average (equal weights for now, can be optimized)\n",
        "        avg_outputs = torch.stack(ensemble_outputs).mean(0)\n",
        "        preds = avg_outputs.argmax(dim=1)\n",
        "\n",
        "        correct += (preds == batch_y).sum().item()\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXPERIMENT PIPELINE FOR 99% TARGET\n",
        "# ============================================================================\n",
        "\n",
        "def run_advanced_experiments():\n",
        "    \"\"\"Run advanced experiments targeting 99% accuracy\"\"\"\n",
        "\n",
        "    for dataset_name in DATASET_NAMES:\n",
        "        print(f\"\\n{'='*90}\")\n",
        "        print(f\"RUNNING ADVANCED SOTA EXPERIMENTS ON {dataset_name}\")\n",
        "        print(f\"TARGET: 99% ACCURACY WITH ADVANCED TECHNIQUES\")\n",
        "        print(f\"{'='*90}\")\n",
        "\n",
        "        train_loader, test_loader, input_dim, n_classes = get_advanced_data_loaders(dataset_name, BATCH_SIZE)\n",
        "        print(f\"Dataset: {dataset_name} | Input: {input_dim} | Classes: {n_classes}\")\n",
        "        print(f\"Train: {len(train_loader.dataset)} | Test: {len(test_loader.dataset)}\")\n",
        "        print(f\"Advanced Augmentation: Enabled | TTA: {TTA_AUGMENTS} augments\")\n",
        "        print_gpu_utilization()\n",
        "\n",
        "        models, individual_accuracies = train_diverse_ensemble(\n",
        "            input_dim, HIDDEN_DIM, n_classes, train_loader, test_loader, ENSEMBLE_SIZE\n",
        "        )\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"INDIVIDUAL MODEL RESULTS\")\n",
        "        print(f\"{'='*70}\")\n",
        "        for i, acc in enumerate(individual_accuracies):\n",
        "            print(f\"Model {i+1}: {acc:.2f}%\")\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ENSEMBLE RESULTS WITH TTA\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Ensemble without TTA\n",
        "        ensemble_acc_no_tta = advanced_ensemble_predict(models, test_loader, use_tta=False)\n",
        "        print(f\"Ensemble Accuracy (no TTA): {ensemble_acc_no_tta:.2f}%\")\n",
        "\n",
        "        # Ensemble with TTA\n",
        "        ensemble_acc_tta = advanced_ensemble_predict(models, test_loader, use_tta=True)\n",
        "        print(f\"Ensemble Accuracy (with TTA): {ensemble_acc_tta:.2f}%\")\n",
        "\n",
        "        final_accuracy = max(max(individual_accuracies), ensemble_acc_no_tta, ensemble_acc_tta)\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"FINAL ADVANCED SOTA RESULTS\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Best Individual Model: {max(individual_accuracies):.2f}%\")\n",
        "        print(f\"Best Ensemble (no TTA): {ensemble_acc_no_tta:.2f}%\")\n",
        "        print(f\"Best Ensemble (with TTA): {ensemble_acc_tta:.2f}%\")\n",
        "        print(f\"FINAL BEST ACCURACY: {final_accuracy:.2f}%\")\n",
        "\n",
        "        if final_accuracy >= 99.0:\n",
        "            print(f\"üéâüéâ OUTSTANDING! {final_accuracy:.2f}% ‚â• 99% TARGET ACHIEVED!\")\n",
        "        elif final_accuracy >= 97.0:\n",
        "            print(f\"üéâ EXCELLENT! {final_accuracy:.2f}% ‚â• 97%\")\n",
        "        elif final_accuracy >= 95.0:\n",
        "            print(f\"‚úÖ VERY GOOD! {final_accuracy:.2f}% ‚â• 95%\")\n",
        "        elif final_accuracy >= 92.0:\n",
        "            print(f\"üü° GOOD! {final_accuracy:.2f}% ‚â• 92%\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è NEEDS IMPROVEMENT: {final_accuracy:.2f}%\")\n",
        "\n",
        "def main():\n",
        "    SEED_ = 42\n",
        "    print(\"=\"*90)\n",
        "    print(\"ADVANCED SOTA FASHIONMNIST IMPLEMENTATION FOR 99% TARGET\")\n",
        "    print(\"FEATURES: Hybrid CNN-Transformer, Advanced Augmentation, TTA, Knowledge Distillation\")\n",
        "    print(\"=\"*90)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Ensemble Size: {ENSEMBLE_SIZE}\")\n",
        "    print(f\"Epochs per Model: {EPOCHS_PER_DATASET}\")\n",
        "    print(f\"Hidden Dim: {HIDDEN_DIM}\")\n",
        "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "    print(f\"Mixed Precision: Enabled\")\n",
        "    print(f\"Test-Time Augmentation: {TTA_AUGMENTS} augments\")\n",
        "    print(f\"Advanced Features: Hybrid Architecture, Stochastic Depth, Knowledge Distillation\")\n",
        "\n",
        "    torch.manual_seed(SEED_)\n",
        "    np.random.seed(SEED_)\n",
        "    random.seed(SEED_)\n",
        "\n",
        "    # Enable optimizations\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "\n",
        "    run_advanced_experiments()\n",
        "    print(\"\\nüéâ ADVANCED SOTA EXPERIMENTS COMPLETED!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NADzoKG0PpIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}